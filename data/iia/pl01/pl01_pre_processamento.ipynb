{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar dependências"
      ],
      "metadata": {
        "id": "wvkIbb9t4cG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBJETIVO:**\n",
        "Garantir que todas as bibliotecas necessárias ao pré-processamento de texto PT estão disponíveis no runtime.\n",
        "\n",
        "**Passos:**\n",
        "  - Instala spaCy (processamento de linguagem natural) e o modelo PT pequeno\n",
        "    'pt_core_news_sm' (suficiente para tokenização e lematização).\n",
        "  - Instala utilitários de limpeza de texto:\n",
        "      * ftfy .......... corrige problemas de codificação Unicode\n",
        "      * unidecode ..... (opcional) remove acentos (normalização)\n",
        "      * wordcloud ..... visualização rápida da \"nuvem\" de termos\n",
        "      * matplotlib .... gráficos simples (barras/wordcloud)\n",
        "      * pandas ........ manipulação tabular dos textos\n",
        "      * scikit-learn .. TF-IDF e vetorização\n",
        "      * lxml .......... utilitários de parsing (não usados diretamente aqui)"
      ],
      "metadata": {
        "id": "MdMdOkvJKf6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGIi0XPf4XmF"
      },
      "outputs": [],
      "source": [
        "!pip -q install spacy ftfy unidecode wordcloud matplotlib pandas scikit-learn lxml\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo:**\n",
        "Definir um pipeline de pré-processamento textual consistente e seguro para Português, do \"texto cru\" até tokens lematizados. Transformar textos PT heterogéneos num formato normalizado e informativo, pronto para vetorização (TF-IDF) e visualização.\n",
        "\n",
        "**Passos:**\n",
        "  1) Carregamento do spaCy PT com parser/NER desativados:\n",
        "     - nlp = spacy.load(\"pt_core_news_sm\", disable=[\"parser\",\"ner\"])\n",
        "     - Motivo: acelerar; aqui só precisamos de tokenização + lemas.\n",
        "\n",
        "  2) Conjuntos de exclusão (ruído):\n",
        "     - PT_STOP .......... stopwords padrão do spaCy para Português.\n",
        "     - DOMAIN_TRASH ..... termos editoriais/HTML comuns (\"ldquo\",\"ndash\", etc.).\n",
        "     - ENTITIES_RE ...... regex para remover entidades HTML (&nbsp;).\n",
        "\n",
        "  3) Função basic_clean(text, lower=True, strip_accents=False, keep_hyphen=True)\n",
        "     - Remove tags HTML residuais e URLs.\n",
        "     - Mantém hífens quando 'keep_hyphen=True' (p.ex. \"primeiro-ministro\"),para evitar quebrar compostos relevantes em PT.\n",
        "     - Normaliza espaços e, se 'lower=True', passa a minúsculas.\n",
        "     - Se 'strip_accents=True', usa unidecode para remover acentos (atenção:\n",
        "       em PT pode alterar significado; por isso está a False por defeito).\n",
        "\n",
        "  4) Função pre_clean(s)\n",
        "     - Corrige Unicode com ftfy (arruma textos \"partidos\" por encoding).\n",
        "     - Dupla decodificação HTML (html.unescape), depois remove entidades com ENTITIES_RE.\n",
        "     - Aplica basic_clean (minúsculas, sem strip de acentos e mantendo hífen).\n",
        "     - Resultado: string limpa, ainda NÃO tokenizada.\n",
        "\n",
        "  5) Função spacy_tokenize_lemmatize(doc_text, remove_stop=True, only_alpha=True, min_len=3)\n",
        "     - Tokeniza com spaCy e usa o LEMA (forma canónica) de cada token.\n",
        "     - Filtros aplicados:\n",
        "          * remove_stop=True  -> remove stopwords (ruído de função)\n",
        "          * only_alpha=True   -> mantém só tokens alfabéticos (descarta nºs)\n",
        "          * min_len=3         -> descarta tokens muito curtos (ruído)\n",
        "     - Remove também itens do DOMAIN_TRASH.\n",
        "     - Resultado: lista de lemas \"limpos\" prontos a análise.\n",
        "\n",
        "**Porquê:**\n",
        "  - A qualidade do texto de entrada determina a qualidade das features.\n",
        "  - Lematizar reduz variação morfológica (p.ex. \"doentes\"/\"doente\" → \"doente\"),e melhora a frequência e peso nos modelos bag-of-words/TF-IDF.\n",
        "  - Os filtros evitam que pontuação, palavras vazias ou \"lixo editorial\" dominem."
      ],
      "metadata": {
        "id": "mbVUpcZa-TSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, html, json\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from ftfy import fix_text\n",
        "from unidecode import unidecode\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# carregar modelo PT do spaCy (removemos o parser e o ner, para efeitos do exemplo)\n",
        "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# stopwords do spaCy (PT); podem ser adicionadas mais\n",
        "PT_STOP = nlp.Defaults.stop_words\n",
        "\n",
        "# remoção de tokens “lixo” comuns de HTML/editoriais\n",
        "DOMAIN_TRASH = {\"ldquo\",\"rdquo\",\"laquo\",\"raquo\",\"ndash\",\"mdash\",\"hellip\",\"apos\",\"video\",\"galeria\",\"podcast\"}\n",
        "\n",
        "ENTITIES_RE = re.compile(r\"&(#\\d+|#x[0-9A-Fa-f]+|[A-Za-z0-9]+);\")\n",
        "\n",
        "def basic_clean(text: str,\n",
        "                lower=True,\n",
        "                strip_accents=False,\n",
        "                keep_hyphen=True) -> str:\n",
        "    \"\"\"Limpeza canónica simples (sem quebrar PT).\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = \"\" if text is None else str(text)\n",
        "    t = text\n",
        "\n",
        "    # remover tags html que tenham escapado\n",
        "    t = re.sub(r\"<[^>]+>\", \" \", t)\n",
        "\n",
        "    # opcional: remover urls\n",
        "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)\n",
        "\n",
        "    # manter hífen se pedido (ex.: 'primeiro-ministro')\n",
        "    if keep_hyphen:\n",
        "        t = re.sub(r\"[^0-9A-Za-zÀ-ÿ\\- ]\", \" \", t)\n",
        "    else:\n",
        "        t = re.sub(r\"[^0-9A-Za-zÀ-ÿ ]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    if lower:\n",
        "        t = t.lower()\n",
        "    if strip_accents:\n",
        "        t = unidecode(t)\n",
        "    return t\n",
        "\n",
        "def pre_clean(s: str) -> str:\n",
        "    \"\"\"Corrigir unicode + decodificar entidades HTML + limpar.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        s = \"\" if s is None else str(s)\n",
        "    t = fix_text(s)\n",
        "    t = html.unescape(t); t = html.unescape(t)  # forçar duas iterações\n",
        "    t = ENTITIES_RE.sub(\" \", t)\n",
        "    t = basic_clean(t, lower=True, strip_accents=False, keep_hyphen=True)\n",
        "    return t\n",
        "\n",
        "def spacy_tokenize_lemmatize(doc_text: str,\n",
        "                             remove_stop=True,\n",
        "                             only_alpha=True,\n",
        "                             min_len=3) -> list[str]:\n",
        "    \"\"\"Tokeniza com spaCy e devolve **lemas** filtrados.\"\"\"\n",
        "    doc = nlp(doc_text)\n",
        "    out = []\n",
        "    for tok in doc:\n",
        "        lemma = tok.lemma_ if tok.lemma_ != \"\" else tok.text\n",
        "        if remove_stop and lemma in PT_STOP:\n",
        "            continue\n",
        "        if only_alpha and not lemma.isalpha():\n",
        "            continue\n",
        "        if len(lemma) < min_len:\n",
        "            continue\n",
        "        out.append(lemma)\n",
        "    # remove lixo editorial\n",
        "    out = [w for w in out if w not in DOMAIN_TRASH]\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "X65AGKAE-dLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBJETIVO:**\n",
        "Criar um DataFrame simples com 2 documentos (\"titulo\", \"texto\") para demonstrar a pipeline. Mostra também como substituir por dados reais (CSV/JSON), que contenham as mesmas colunas. Ter o DataFrame de partida pronto para limpeza/tokenização.\n",
        "\n",
        "**O QUE FAZ:**\n",
        "  * Cria df = pd.DataFrame([...]) com colunas 'titulo' e 'texto'.\n",
        "\n",
        "**Passos:**\n",
        "  1) O pipeline assume a coluna 'texto' como fonte principal; 'titulo' é opcional e serve para rotular linhas (mais legível nos outputs).\n",
        "\n",
        "**Notas**\n",
        "  * Este bloco também mostra a \"construção de top-k TF-IDF por documento\" e tentativa de exportar CSV.\n",
        "  * Se aparecer NameError: 'out_dir' não definido, define antes:\n",
        "       from pathlib import Path\n",
        "       out_dir = Path(\"sample_data\"); out_dir.mkdir(exist_ok=True, parents=True)\n",
        "e garante que a variável 'row_index' existe."
      ],
      "metadata": {
        "id": "1DNf-8qpMkAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXEMPLO mínimo com 2 textos:\n",
        "data_example = [\n",
        "    {\"titulo\": \"Parlamento aprova medida\",\n",
        "     \"texto\": \"O Parlamento deu luz verde à proposta. Nesse parlamento, Gabriel Bernardino falou à imprensa em Lisboa.\"},\n",
        "    {\"titulo\": \"ASF anuncia nova orientação\",\n",
        "     \"texto\": \"A Autoridade de Supervisão de Seguros e Fundos de Pensões juntamente com o Parlamento publicou hoje novas regras.\"}\n",
        "]\n",
        "df = pd.DataFrame(data_example)\n",
        "\n",
        "# PARA USAR OUTROS DATASETS:\n",
        "#   a) Carregar CSV:\n",
        "# df = pd.read_csv(\"/content/ficheiro.csv\")  # garantir que tem as colunas: 'titulo' e 'texto'\n",
        "#   b) Carregar JSON (lista de objetos):\n",
        "# df = pd.read_json(\"/content/ficheiro.json\")\n",
        "\n",
        "# detetar colunas prováveis\n",
        "prefer_text = [\"texto\",\"noticia\",\"content\",\"body\",\"descricao\",\"description\"]\n",
        "prefer_title = [\"titulo\",\"title\",\"headline\"]\n",
        "cols = {c.lower(): c for c in df.columns}\n",
        "text_col = next((cols[c] for c in prefer_text if c in cols), None)\n",
        "title_col = next((cols[c] for c in prefer_title if c in cols), None)\n",
        "if text_col is None:\n",
        "    # fallback: primeira coluna de strings\n",
        "    text_col = df.select_dtypes(include=[\"object\"]).columns[0]\n",
        "df = df[[c for c in [title_col, text_col] if c is not None]].dropna().drop_duplicates()\n",
        "df = df.rename(columns={title_col or \"\": \"titulo\", text_col: \"texto\"})\n",
        "print(df.head(2))\n",
        "print(f\"docs: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "hWFk2foPDDcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo:**\n",
        "Aplicar as funções definidas atrás para obter:\n",
        "* 'clean_text' ......... texto normalizado (string)\n",
        "* 'tokens' ............. lista de lemas filtrados por doc\n",
        "Obter uma representação por documento que alimenta visualizações e TF-IDF.\n",
        "\n",
        "\n",
        "**Passos:**  \n",
        "* df[\"clean_text\"] = df[\"texto\"].apply(pre_clean)\n",
        "* df[\"tokens\"] = df[\"clean_text\"].apply(lambda t: spacy_tokenize_lemmatize(\n",
        "    t, remove_stop=True, only_alpha=True, min_len=3))\n",
        "* Imprime exemplo de tokens do primeiro documento (sanity check).\n",
        "\n",
        "**Porquê:**\n",
        "* Confirmar que as escolhas (stopwords, only_alpha, min_len) estão a produzir um vocabulário limpo e coerente para o teu domínio.\n"
      ],
      "metadata": {
        "id": "kEOKTqTCNYIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# limpeza\n",
        "df[\"clean_text\"] = df[\"texto\"].apply(pre_clean)\n",
        "\n",
        "# tokenização + lematização\n",
        "df[\"tokens\"] = df[\"clean_text\"].apply(lambda t: spacy_tokenize_lemmatize(\n",
        "    t, remove_stop=True, only_alpha=True, min_len=3\n",
        "))\n",
        "\n",
        "# inspeção\n",
        "print(\"Exemplo tokens do documento 0:\", df[\"tokens\"].iloc[0][:20])"
      ],
      "metadata": {
        "id": "5hV8F6ZrDSv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo:**\n",
        "Análise exploratória do vocabulário resultante. Validar rapidamente a qualidade do vocabulário e ajustar filtros se preciso.\n",
        "\n",
        "**Passos:**\n",
        "* 'all_tokens' concatena tokens de todos os docs e calcula frequências (Counter).\n",
        "* Imprime tamanho do vocabulário e top-20 termos.\n",
        "* Desenha gráfico de barras com top-K termos (K = min(25, |voc|)).\n",
        "* Gera uma wordcloud do corpus limpo.\n",
        "\n",
        "**Porquê:**\n",
        "* Perceber se o pré-processamento removeu ruído suficiente e se os termos com maior frequência fazem sentido no contexto (sanity check).\n",
        "\n",
        "**Limitações:**\n",
        "* Wordcloud e contagens “cruas” podem enviesar perceção (termos longos ou muito frequentes dominam visualmente). Para importância por documento, preferir TF-IDF.\n"
      ],
      "metadata": {
        "id": "GM7J_hLIONuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualização global\n",
        "all_tokens = [w for doc in df[\"tokens\"] for w in doc]\n",
        "freq = Counter(all_tokens)\n",
        "print(\"Vocabulário:\", len(freq))\n",
        "print(\"Top 20 termos:\", freq.most_common(20))\n",
        "\n",
        "# gráfico de barras (top-K)\n",
        "K = min(25, len(freq))\n",
        "if K > 0:\n",
        "    labels, values = zip(*freq.most_common(K))\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.bar(labels, values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(\"Top termos (frequência)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# wordcloud\n",
        "if len(all_tokens) > 0:\n",
        "    wc = WordCloud(width=1000, height=500, background_color=\"white\")\n",
        "    wc = wc.generate(\" \".join(all_tokens))\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Nuvem de palavras (corpus)\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6lqPbRZrDr9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo:** Converter cada documento numa representação numérica (vetor TF-IDF), permitindo medir importância relativa de termos por documento. Preparar terreno para ranking de palavras-chave, pesquisa por similaridade,clustering de documentos, etc\n",
        "\n",
        "**Passos:**\n",
        "* 'texts_joined' junta tokens por doc numa string (entrada do TfidfVectorizer).\n",
        "* Cria TF-IDF com max_features=5000 (podes ajustar; ver notas abaixo).\n",
        "* Calcula a DTM (Document-Term Matrix) esparsa X de shape (n_docs, n_terms).\n",
        "* Define função top_tfidf_terms_for_doc(i, k) para retornar os k termos de maior peso TF-IDF no documento i.\n",
        "* Imprime top-10 do documento 0 (exemplo).\n",
        "\n",
        "**Porquê:**\n",
        "* TF (Term Frequency) favorece o que é recorrente no doc.\n",
        "* IDF (Inverse Document Frequency) penaliza termos demasiado comuns no corpus.\n",
        "* O produto destaca termos “característicos” daquele documento.\n",
        "\n",
        "\n",
        "**Notas:**\n",
        "PARÂMETROS ÚTEIS A AFINAR (quando aplicares a corpora maiores):\n",
        "* ngram_range=(1,2) .......... ativa bigramas (pode capturar expressões estáveis)\n",
        "* min_df / max_df ............ remove termos raros/dominantes (ex.: min_df=3, max_df=0.8)\n",
        "* max_features ............... limita vocabulário (eficiência/overfitting)"
      ],
      "metadata": {
        "id": "aUjLPYfIOklx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# juntar os tokens por doc (string) para alimentar o TF-IDF\n",
        "texts_joined = df[\"tokens\"].apply(lambda xs: \" \".join(xs)).tolist()\n",
        "\n",
        "# TF-IDF clássico (unigramas). Ajusta max_features se for necessário limitar.\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(texts_joined)  # shape: (n_docs, n_terms)\n",
        "terms = tfidf.get_feature_names_out()\n",
        "print(\"DTM TF-IDF:\", X.shape)\n",
        "\n",
        "# Top termos por documento (k melhores)\n",
        "def top_tfidf_terms_for_doc(row_index, k=10):\n",
        "    row = X.getrow(row_index)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "    # termos ordenados por peso decrescente\n",
        "    inds = row.indices\n",
        "    vals = row.data\n",
        "    order = vals.argsort()[::-1][:k]\n",
        "    return [(terms[inds[i]], float(vals[i])) for i in order]\n",
        "\n",
        "# exemplo: top-10 do 1º documento\n",
        "print(\"Top-10 TF-IDF (documento 0):\", top_tfidf_terms_for_doc(0, k=10))\n"
      ],
      "metadata": {
        "id": "clfmFz_R37fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo:**\n",
        "Produzir um resumo estruturado dos termos mais relevantes (por TF-IDF) para todos os documentos e guardar em CSV. Deixar em disco um artefacto reutilizável (CSV) com os “top termos por doc”, pronto para integrar noutros sistemas/processos.\n",
        "\n",
        "**Passos:**\n",
        "* out_dir = Path(\"sample_data\") ................ define pasta de saída. (cria manualmente com out_dir.mkdir(exist_ok=True, parents=True) se necessário)\n",
        "* Reajusta o TF-IDF sobre 'texts_joined' (mesma configuração da célula 7).\n",
        "* Define 'row_index' (rótulos das linhas):\n",
        "* Se existir 'titulo' completo, usa-o (truncado a 80 chars para legibilidade); caso contrário, usa \"doc_i\" por índice.\n",
        "* Para cada documento:\n",
        "    * extrai os índices e pesos dos termos não nulos;\n",
        "    * ordena por peso decrescente;\n",
        "    * recolhe os top-k (ex.: k=10) com: doc_idx, doc_label, rank, term, tfidf;\n",
        "* Concatena tudo num DataFrame 'df_topk', mostra um head(20) para inspeção.\n",
        "* Escreve CSV: sample_data/tfidf_top{k}_per_doc.csv\n",
        "\n",
        "**Porquê:**\n",
        "* Gera um “resumo automático” do conteúdo de cada documento sem supervisionar.\n",
        "* Útil para: etiquetas rápidas, pesquisa, triagem manual, criação de dicionários.\n",
        "\n",
        "\n",
        "**Erros Comuns:**\n",
        "* Se 'out_dir' NÃO existir, a escrita do CSV falha → cria a pasta antes.\n",
        "* Se 'row_index' usar 'titulo' com NaN, converte para string segura: row_index = [str(t) if pd.notna(t) else f\"doc_{i}\" for i,t in enumerate(df[\"titulo\"])]\n",
        "* Vocab pequeno + docs curtos podem dar linhas sem termos (row.nnz == 0)."
      ],
      "metadata": {
        "id": "aDpBDVEdFw2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# MATRIZ TF-IDF COMPLETA\n",
        "# =========================\n",
        "from scipy import sparse\n",
        "\n",
        "out_dir = Path(\"sample_data\")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf.fit_transform(texts_joined)   # shape: (n_docs, n_terms)\n",
        "terms = tfidf.get_feature_names_out()\n",
        "\n",
        "print(\"DTM TF-IDF:\", X.shape)\n",
        "\n",
        "# índice de linhas (usa títulos se existirem, senão usa doc_i)\n",
        "if \"titulo\" in df.columns and df[\"titulo\"].notna().all():\n",
        "    row_index = [str(t)[:80] for t in df[\"titulo\"].tolist()]  # corta para ficar legível\n",
        "else:\n",
        "    row_index = [f\"doc_{i}\" for i in range(len(df))]\n",
        "\n",
        "# Converter para denso só se a matriz for pequena\n",
        "max_cells_to_dense = 2000000  # ~2M células; ajustar conforme a RAM\n",
        "n_cells = X.shape[0] * X.shape[1]\n",
        "\n",
        "if n_cells <= max_cells_to_dense:\n",
        "    import numpy as np\n",
        "    df_tfidf = pd.DataFrame(X.toarray(), columns=terms, index=row_index)\n",
        "    display(df_tfidf.round(3).head())         # preview\n",
        "    # guardar CSV da matriz densa\n",
        "    out_csv = out_dir / \"tfidf_matrix_dense.csv\"\n",
        "    df_tfidf.to_csv(out_csv, index=True, encoding=\"utf-8\")\n",
        "    print(\"Matriz TF-IDF densa guardada em:\", out_csv)\n",
        "else:\n",
        "    print(\"Matriz grande — a guardar em formato esparso (.npz) + termos...\")\n",
        "    # guardar matriz esparsa + termos (forma recomendada para grandes)\n",
        "    out_npz = out_dir / \"tfidf_matrix_sparse.npz\"\n",
        "    sparse.save_npz(out_npz, X)\n",
        "    (out_dir / \"tfidf_terms.txt\").write_text(\"\\n\".join(terms), encoding=\"utf-8\")\n",
        "    (out_dir / \"tfidf_rows.txt\").write_text(\"\\n\".join(row_index), encoding=\"utf-8\")\n",
        "    print(\"Guardado:\", out_npz, \"(e tfidf_terms.txt, tfidf_rows.txt)\")\n",
        "\n",
        "# =========================\n",
        "# TOP-k TERMOS POR DOCUMENTO (sem densificar)\n",
        "# =========================\n",
        "def top_tfidf_terms_for_doc(row_index_int: int, k: int = 10):\n",
        "    row = X.getrow(row_index_int)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "    inds = row.indices\n",
        "    vals = row.data\n",
        "    order = vals.argsort()[::-1][:k]\n",
        "    return [(terms[inds[i]], float(vals[i])) for i in order]\n",
        "\n",
        "# construir tabela com top-k para TODOS os docs\n",
        "k = 10\n",
        "rows = []\n",
        "for i in range(X.shape[0]):\n",
        "    tops = top_tfidf_terms_for_doc(i, k=k)\n",
        "    for rank, (term, score) in enumerate(tops, start=1):\n",
        "        rows.append({\n",
        "            \"doc_idx\": i,\n",
        "            \"doc_label\": row_index[i],\n",
        "            \"rank\": rank,\n",
        "            \"term\": term,\n",
        "            \"tfidf\": score\n",
        "        })\n",
        "\n",
        "df_topk = pd.DataFrame(rows)\n",
        "display(df_topk.head(20))\n",
        "\n",
        "# guardar top-k em CSV\n",
        "out_topk = out_dir / f\"tfidf_top{str(k)}_per_doc.csv\"\n",
        "df_topk.to_csv(out_topk, index=False, encoding=\"utf-8\")\n",
        "print(\"Top-k por documento guardado em:\", out_topk)"
      ],
      "metadata": {
        "id": "_7AvKWlnEuUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}