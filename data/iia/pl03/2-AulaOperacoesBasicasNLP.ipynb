{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9VRpsCCEM2Y"
      },
      "source": [
        "### Normalização\n",
        "\n",
        "A normalização de texto é uma etapa essencial no pré-processamento de dados textuais para aplicações em Machine Learning. Ela procura padronizar o conteúdo, eliminando variações como letras maiúsculas/minúsculas e formas no singular/plural que não alteram o significado da informação. Isso garante que o modelo interprete variações equivalentes de maneira uniforme, facilitando a aprendizagem e melhorando a performance. Um exemplo comum de normalização é converter todas as palavras para letras minúsculas, tratando “NLP” e “nlp” como a mesma entidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcOclBlqEFLp"
      },
      "outputs": [],
      "source": [
        "text = \"Estou a estudar NLP\"\n",
        "text = text.lower()\n",
        "#text: \"estou a estudar nlp\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbEWWUqkEf8E"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "O Stemming é uma técnica de normalização que reduz palavras à sua raiz comum, como \"estudar\" e “estudo” que são transformadas em “estud”. Apesar de gerar formas não necessariamente legíveis, essa simplificação ajuda a uniformizar o texto e preservar apenas a informação essencial para o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD9D4egWFESA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Seleciona o idioma português para o stemmer\n",
        "snow_stemmer = SnowballStemmer(language='portuguese')\n",
        "#Quebra o text original em uma lista de plavras\n",
        "text = \"Estamos a ir para as nossas casas hoje\"\n",
        "words = text.split()\n",
        "#Aplica o stemmer em cada palavra\n",
        "stemmed_words = [snow_stemmer.stem(w) for w in words]\n",
        "# Une a listsa de palavras em uma string única\n",
        "stemmed_text = \" \".join(stemmed_words)\n",
        "#stemmed_text: \"estam a ir par as noss cas hoj\"\n",
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLDWztC1Ff_j"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Stopwords são palavras muito frequentes e pouco informativas, como artigos e preposições, que geralmente são removidas do texto para evitar ruído nos modelos. No entanto, é preciso atenção, pois palavras como “não” também costumam estar nesta lista e podem carregar significado importante. Nesse caso, é recomendável ajustar a lista de stopwords para manter termos relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI5ThgkkFg0y"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "text = \"Estamos a ir para as nossas casas hoje\"\n",
        "stop_words = set(stopwords.words(\"portuguese\"))\n",
        "words = text.split()\n",
        "text_without_stopwords_list = [w for w in words if not w in stop_words]\n",
        "text_without_stopwords = \" \".join(text_without_stopwords_list)\n",
        "#text_without_stopwords: \"Estamos ir nossas casas hoje\"\n",
        "print(text_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-IJo4gLGVeE"
      },
      "source": [
        "### Remoção de Pontuação\n",
        "\n",
        "Pode-se eliminar também sinais de pontuação, como “!”, “?” ou “.”, que em muitas aplicações não são informações relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er7zCtpWGcCt"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "text = \"Estamos a ir para as nossas casas hoje!\"\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text_without_punct = text.translate(translator)\n",
        "# text_without_punct: “Estamos a ir para as nossas casas hoje\"\n",
        "\n",
        "print(text_without_punct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf-OMuzCG8VS"
      },
      "source": [
        "### Números e Caracteres Especiais\n",
        "\n",
        "Números e caracteres especiais geralmente são removidos ou substituídos por tags (como <NUM>) durante a normalização, especialmente quando não agregam valor à análise. No entanto, esta decisão depende do contexto da aplicação: se os números forem relevantes, devem ser mantidos. Nem todos os modelos lidam bem com tags personalizadas, por isso a escolha do tratamento deve ser feita com critério.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOcVjDh4HEzp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "text_num_tag = re.sub(r\"\\d+\", \"<NUM>\", text)\n",
        "# text_num_tag: Estudo NLP há <NUM> dias\"\n",
        "print(text_num_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiwpl_z2HeWU"
      },
      "source": [
        "### POS Tagging (Part-of-Speech Tagging)\n",
        "\n",
        "POS tagging (Part-of-Speech tagging) é o processo de identificar e rotular automaticamente as classes gramaticais das palavras num texto, como substantivos, verbos, adjetivos, etc. Essa etapa é essencial para análises linguísticas mais precisas, permitindo compreender a função de cada palavra na frase e facilitando tarefas como análise sintática, desambiguação de sentidos e extração de informações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlnAyWiIWuL"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Carrega o modelo após o download\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.pos_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGBCsmR4I3vs"
      },
      "source": [
        "### Dependency Parsing\n",
        "\n",
        "Dependency parsing é a análise das dependências sintáticas entre as palavras de uma frase, identificando qual palavra é a principal (head) e como as outras se relacionam a ela. O resultado é uma estrutura em forma de árvore que revela a hierarquia e as funções gramaticais no enunciado, sendo fundamental para entender a estrutura e o significado da frase em tarefas como tradução automática, extração de relações e compreensão de linguagem natural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEFduhi8I-6q"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Carrega o modelo de português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Exibe informações de dependência para cada token\n",
        "for token in doc:\n",
        "    print(f\"{token.text:10} | POS: {token.pos_:6} | DEP: {token.dep_:10} | HEAD: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io1Ie_OOJ2gT"
      },
      "source": [
        "### Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition (NER) é a tarefa de identificar e classificar automaticamente entidades mencionadas num texto, como nomes de pessoas, organizações, locais, datas e valores. Por exemplo, em “Microsoft é uma empresa de tecnologia fundada em Albuquerque”, o modelo reconhece “Microsoft” como organização e “Albuquerque” como local. NER é amplamente suportado por bibliotecas como SpaCy e serviços cloud como o Amazon Comprehend, inclusive para português."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9ljDpvqJ3zi"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pt_core_news_sm\n",
        "nlp = pt_core_news_sm.load()\n",
        "doc = nlp('Microsoft é uma empresa fundada em Albuquerque')\n",
        "print([(X.text, X.label_) for X in doc.ents])\n",
        "# Output:\n",
        "# [('Microsoft', 'ORG'), ('Albuquerque', 'LOC')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Exemplo para um texto mais complexo da aplicação de NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_lg\n",
        "\n",
        "import spacy\n",
        "import pt_core_news_lg  # modelo maior e mais preciso para português\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar o modelo spaCy para português\n",
        "nlp = pt_core_news_lg.load()\n",
        "\n",
        "# Texto de exemplo — múltiplas entidades\n",
        "texto = \"\"\"\n",
        "Em 12 de março de 2024, a Microsoft anunciou em Lisboa uma parceria com a Universidade do Porto\n",
        "para desenvolver soluções de Inteligência Artificial aplicadas à saúde. O investimento inicial será\n",
        "de 2 milhões de euros, financiado parcialmente pelo Governo de Portugal. Satya Nadella afirmou que\n",
        "o projeto reforça o compromisso da empresa com a inovação tecnológica na Europa.\n",
        "\"\"\"\n",
        "\n",
        "# Processar o texto com spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Extrair entidades nomeadas\n",
        "entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Entidades reconhecidas:\")\n",
        "for ent_text, ent_label in entidades:\n",
        "    print(f\" - {ent_text:<30} → {ent_label}\")\n",
        "\n",
        "# Contar número de entidades por tipo\n",
        "contagem = Counter([ent.label_ for ent in doc.ents])\n",
        "print(\"\\n Contagem por tipo de entidade:\")\n",
        "for tipo, qtd in contagem.items():\n",
        "    print(f\" {tipo}: {qtd}\")\n",
        "\n",
        "# Exportar para DataFrame (útil para análises)\n",
        "df_ent = pd.DataFrame(entidades, columns=[\"Entidade\", \"Tipo\"])\n",
        "print(\"\\n Tabela de entidades:\\n\")\n",
        "print(df_ent)\n",
        "\n",
        "# Exemplo de filtro: mostrar apenas ORGANIZAÇÕES\n",
        "orgs = df_ent[df_ent[\"Tipo\"] == \"ORG\"]\n",
        "print(\"\\n Organizações encontradas:\")\n",
        "print(orgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0aLYfZbKGxK"
      },
      "source": [
        "## Summarization\n",
        "\n",
        "Summarization é a tarefa de gerar automaticamente um texto mais curto que preserve as informações mais relevantes de um conteúdo maior. Modelos de summarization são úteis para sintetizar documentos extensos, facilitando a compreensão rápida. Ferramentas como a HuggingFace oferecem modelos eficientes para esse fim, inclusive em português."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Validar a instalação do torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW_KcU-SKHpa"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# cria o pipeline de sumarização\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# texto de exemplo\n",
        "text = (\n",
        "    \"A inteligência artificial (IA) refere-se à simulação de processos de inteligência humana por máquinas, \"\n",
        "    \"especialmente sistemas computacionais. Esses processos incluem aprendizado, raciocínio e autocorreção. \"\n",
        "    \"Aplicações específicas da IA incluem sistemas especialistas, reconhecimento de fala e visão computacional. \"\n",
        "    \"Nos últimos anos, avanços em redes neurais profundas permitiram melhorias significativas em tarefas como \"\n",
        "    \"tradução automática e geração de linguagem natural. Com isso, a IA tem sido cada vez mais utilizada em \"\n",
        "    \"áreas como medicina, finanças, educação e transporte.\"\n",
        ")\n",
        "\n",
        "# sumarizar\n",
        "summary = summarizer(text, max_length=120, min_length=40, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYSJ-x9zKmYj"
      },
      "source": [
        "### Question Answering (QA)\n",
        "\n",
        "Question Answering (QA) é a tarefa em que um modelo compreende um texto e responde perguntas com base nesse conteúdo. É especialmente útil para procura e extração de informações em grandes volumes de dados. Modelos de QA, como os disponíveis no HuggingFace, conseguem localizar respostas precisas diretamente no contexto fornecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ6YI13TKumc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Pipeline com modelo compatível com português\n",
        "question_answerer = pipeline(\"question-answering\", model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\")\n",
        "\n",
        "# Novo contexto\n",
        "context = (\n",
        "    \"A inteligência artificial (IA) é um campo da ciência da computação que busca criar sistemas capazes de realizar tarefas \"\n",
        "    \"que normalmente requerem inteligência humana, como reconhecimento de fala, tomada de decisões, e tradução de idiomas. \"\n",
        "    \"Ela tem sido aplicada em diversos setores, como saúde, educação, transporte e segurança. Com o avanço das redes neurais \"\n",
        "    \"e do aprendizado profundo, a IA passou a desempenhar papéis centrais em tecnologias modernas, incluindo assistentes virtuais \"\n",
        "    \"e veículos autônomos.\"\n",
        ")\n",
        "\n",
        "# Pergunta em português\n",
        "result = question_answerer(question=\"Em quais áreas a IA tem sido aplicada?\", context=context)\n",
        "\n",
        "# Exibe apenas a resposta\n",
        "print(result['answer'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLLM5I4LLDBT"
      },
      "source": [
        "### Tradução Automática\n",
        "\n",
        "Tradução automática é a tarefa de converter textos de um idioma para outro. Modelos modernos são capazes de lidar com diferentes estruturas linguísticas e até traduzir siglas corretamente. Em Python, uma opção prática é usar a API do GoogleTranslator para realizar traduções de forma eficiente e precisa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C83HsG1ILLsq"
      },
      "outputs": [],
      "source": [
        "!pip install deep-translator\n",
        "\n",
        "# 2. Use o tradutor\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "to_translate = (\n",
        "    \"Artificial Intelligence is transforming the way we interact with technology. \"\n",
        "    \"From virtual assistants that respond to voice commands, to recommendation systems \"\n",
        "    \"that help us choose what to watch or buy, AI is becoming an essential part of our daily lives. \"\n",
        "    \"As algorithms become more sophisticated, they are now being used in fields like healthcare, \"\n",
        "    \"education, finance, and autonomous vehicles. However, this rapid development also raises \"\n",
        "    \"ethical concerns and questions about the future of human labor.\"\n",
        ")\n",
        "\n",
        "translated_text = GoogleTranslator(source='auto', target='pt').translate(to_translate)\n",
        "print(translated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYmUVMrPAN6"
      },
      "source": [
        "### Transformar Texto em Feature\n",
        "\n",
        "Modelos de Machine Learning como RandomForest, Logistic Regression, Naive Bayes e SVM (Support Vector Machine), entre outros disponíveis no Scikit-learn, exigem que os dados de entrada estejam em formato numérico. Como textos não estruturados não podem ser diretamente convertidos em float, é necessário aplicar técnicas de vetorização, como Bag of Words, TF-IDF ou Word Embeddings (por exemplo, Word2Vec ou GloVe), para transformar palavras em números. Algumas dessas abordagens preservam o contexto semântico, outras apenas a frequência. A escolha da técnica depende do objetivo da modelagem. O dataset “20 newsgroups” é um exemplo popular usado para treinar e avaliar esses modelos com dados textuais classificados por temas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTvSWbT0POfL"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Carrega os dados de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=2)\n",
        "\n",
        "# Exemplo: Visualização de uma amostra\n",
        "print(f\"Total de documentos: {len(twenty_train.data)}\")\n",
        "print(f\"Categorias: {twenty_train.target_names}\")\n",
        "print(\"\\nExemplo de documento:\")\n",
        "print(twenty_train.data[0][:500])  # primeiros 500 caracteres do primeiro documento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8mwFIHPxPD"
      },
      "source": [
        "### Bag-of-Words\n",
        "\n",
        "Bag-of-Words (BoW) é a técnica mais simples para transformar texto em features. É criada uma tabela onde cada coluna representa uma palavra do vocabulário e cada linha representa um documento, com valores indicando a frequência de cada palavra no texto. Embora não preserve o contexto ou a ordem das palavras, é eficaz em muitas aplicações. No Scikit-learn, essa transformação é feita com a classe CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP_afQodP8V7"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Inicializa o CountVectorizer (Bag of Words)\n",
        "count_vect = CountVectorizer(stop_words='english')  # remove stopwords em inglês\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "\n",
        "# Exibe informações\n",
        "print(\"Formato da matriz de vetores (documentos x palavras):\", X_train_counts.shape)\n",
        "print(\"\\nExemplo de palavras no vocabulário:\")\n",
        "print(count_vect.get_feature_names_out()[:20])  # mostra as 20 primeiras palavras\n",
        "print(\"\\nExemplo de documento:\")\n",
        "print(twenty_train.data[0][:500])  # mostra os primeiros 500 caracteres do 1º documento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjhi4fyF8DlW"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency) é uma técnica de vetorização que melhora o Bag-of-Words ao normalizar a frequência das palavras pelo tamanho do texto e ao reduzir o peso de termos muito comuns na base. Isso evita que textos mais longos tenham influência desproporcional e destaca palavras mais relevantes. Embora ainda não capture o contexto, é bastante eficaz para diversas tarefas. No Scikit-learn, a classe TfidfVectorizer realiza essa transformação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPYJ4sEVQqn6"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Inicializa o vetor TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')  # remove stopwords em inglês\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
        "\n",
        "# Exibe informações sobre a matriz\n",
        "print(\"Formato da matriz TF-IDF (documentos x palavras):\", X_train_tfidf.shape)\n",
        "\n",
        "# Exibe palavras do vocabulário\n",
        "print(\"\\nExemplo de termos no vocabulário:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:20])\n",
        "\n",
        "# Exemplo de valores TF-IDF (linha 0, primeiros 10 termos não-nulos)\n",
        "print(\"\\nExemplo de vetores TF-IDF do primeiro documento (valores não-nulos):\")\n",
        "import numpy as np\n",
        "row_0 = X_train_tfidf[0].tocoo()\n",
        "for i, j, v in zip(row_0.row, row_0.col, row_0.data):\n",
        "    print(f\"{tfidf_vectorizer.get_feature_names_out()[j]}: {v:.4f}\")\n",
        "    if i >= 10:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Melhorar o output ao aplicar algumas técnicas de limpeza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instala scikit-learn se estiver no Google Colab\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ---------- LIMPEZA BÁSICA ----------\n",
        "def clean_text(text):\n",
        "    text = text.lower()                          # tudo em minúsculas\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)       # remove emails\n",
        "    text = re.sub(r'http\\S+', '', text)          # remove URLs\n",
        "    text = re.sub(r'\\d+', '', text)              # remove números\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)      # remove caracteres não alfabéticos\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()     # remove espaços extras\n",
        "    return text\n",
        "\n",
        "# ---------- TOKENIZER PERSONALIZADO ----------\n",
        "def custom_tokenizer(text):\n",
        "    tokens = re.findall(r'\\b[a-zA-Z]{2,}\\b', text.lower())\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if len(token) > 20:\n",
        "            continue  # descarta tokens longos\n",
        "        token = re.sub(r'(.)\\1{2,}', r'\\1\\1', token)  # reduz repetições (aaaaah -> aah)\n",
        "        clean_tokens.append(token)\n",
        "    return clean_tokens\n",
        "\n",
        "# ---------- CARREGAR E LIMPAR DADOS ----------\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "cleaned_data = [clean_text(doc) for doc in twenty_train.data]\n",
        "\n",
        "# ---------- VETORIZAÇÃO INICIAL ----------\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=custom_tokenizer)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(cleaned_data)\n",
        "\n",
        "print(\"Formato inicial da matriz TF-IDF:\", X_train_tfidf.shape)\n",
        "\n",
        "# ---------- DETETAR TOKENS SUSPEITOS ----------\n",
        "vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "weird_tokens = [w for w in vocab if (\n",
        "    len(w) > 15 or                # palavras muito longas\n",
        "    w.count(w[0]) > len(w) // 2   # uma só letra repetida (aaaaa, bbbbb)\n",
        ")]\n",
        "\n",
        "print(f\"\\nForam encontrados {len(weird_tokens)} tokens suspeitos (exemplo):\")\n",
        "print(weird_tokens[:15])\n",
        "\n",
        "# ---------- RECRIAR MODELO SEM OS TOKENS SUSPEITOS ----------\n",
        "filtered_vocab = [w for w in vocab if w not in weird_tokens]\n",
        "\n",
        "# Refaz o vetor TF-IDF restringindo o vocabulário\n",
        "tfidf_vectorizer_clean = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    tokenizer=custom_tokenizer,\n",
        "    vocabulary=filtered_vocab\n",
        ")\n",
        "X_train_tfidf_clean = tfidf_vectorizer_clean.fit_transform(cleaned_data)\n",
        "\n",
        "print(\"\\nFormato final da matriz TF-IDF (após limpeza):\", X_train_tfidf_clean.shape)\n",
        "\n",
        "# ---------- MOSTRAR RESULTADOS ----------\n",
        "print(\"\\nExemplo de termos limpos no vocabulário:\")\n",
        "print(tfidf_vectorizer_clean.get_feature_names_out()[:20])\n",
        "\n",
        "print(\"\\nExemplo de valores TF-IDF do primeiro documento (valores não-nulos):\")\n",
        "row_0 = X_train_tfidf_clean[0].tocoo()\n",
        "for i, j, v in zip(row_0.row, row_0.col, row_0.data):\n",
        "    print(f\"{tfidf_vectorizer_clean.get_feature_names_out()[j]}: {v:.4f}\")\n",
        "    if i >= 10:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_OeQFGRIpJ"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) é um modelo pré-treinado desenvolvido pela Google que transforma textos em features mantendo o contexto e o significado das palavras nas frases. Baseado na arquitetura Transformer, o BERT é capaz de compreender relações semânticas complexas e é amplamente usado em tarefas avançadas de NLP. Existem várias versões do BERT, e uma das mais comuns é a bert-base-uncased, disponível pela biblioteca HuggingFace. É uma das abordagens mais eficazes para representações contextuais de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzm52t2ERPla"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Exemplo de corpus em português\n",
        "corpus = [\n",
        "    \"A inteligência artificial está transformando o mundo.\",\n",
        "    \"Sistemas de IA já são usados na medicina, educação e transporte.\",\n",
        "    \"Receitas culinárias brasileiras são ricas em sabores e temperos.\",\n",
        "    \"A tecnologia está cada vez mais presente na vida cotidiana.\"\n",
        "]\n",
        "\n",
        "# Carrega modelo multilíngue para embeddings semânticos\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Gera embeddings BERT\n",
        "embeddings = model.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "# Calcula matriz de similaridade entre os textos\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Mostra a matriz de similaridade\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_sim = pd.DataFrame(similarity_matrix, columns=[f\"Texto {i}\" for i in range(len(corpus))],\n",
        "                      index=[f\"Texto {i}\" for i in range(len(corpus))])\n",
        "print(df_sim.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2mGWTCcSO2q"
      },
      "source": [
        "### Aplicar modelos de Machine Learning\n",
        "\n",
        "Após transformar o texto em features (como com TF-IDF), o processo de construção de modelos de Machine Learning segue o mesmo fluxo usado com dados estruturados. É possível, por exemplo, treinar um RandomForestClassifier diretamente com os vetores gerados. Para prever novas entradas, basta aplicar a mesma transformação com o objeto previamente ajustado (transform, e não fit_transform). Essa abordagem é válida para qualquer modelo supervisionado, e técnicas como ajuste de parâmetros e avaliação de métricas também se aplicam normalmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BGcbiLEuS1Z6"
      },
      "outputs": [],
      "source": [
        "# Instale bibliotecas necessárias (se estiver no Colab)\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "print(\"Carregando dataset...\")\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Exibe as categorias disponíveis\n",
        "print(\"\\nCategorias disponíveis:\")\n",
        "for i, cat in enumerate(twenty_train.target_names):\n",
        "    print(f\"{i:2d} - {cat}\")\n",
        "\n",
        "# Transforma os textos em features TF-IDF\n",
        "print(\"\\nTransformando textos em vetores TF-IDF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
        "\n",
        "# Treina o classificador\n",
        "print(\"Treinando modelo RandomForest...\")\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_tfidf, twenty_train.target)\n",
        "print(f\"Número de árvores no modelo: {len(clf.estimators_)}\")\n",
        "\n",
        "# Função para testar entrada do usuário\n",
        "def classificar_texto(texto):\n",
        "    X_novo = tfidf_vectorizer.transform([texto])\n",
        "    predicao = clf.predict(X_novo)[0]\n",
        "    return twenty_train.target_names[predicao]\n",
        "\n",
        "# Loop de teste interativo\n",
        "print(\"\\nDigite um texto (ou pressione Enter para sair):\")\n",
        "while True:\n",
        "    entrada = input(\"\\nTexto: \")\n",
        "    if not entrada.strip():\n",
        "        print(\"Encerrado.\")\n",
        "        break\n",
        "    categoria = classificar_texto(entrada)\n",
        "    print(f\"→ Categoria prevista: {categoria}\")\n",
        "\n",
        "\n",
        "# The government announced a new economic policy\n",
        "# I love computers\n",
        "# My car engine is overheating after long trips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_HLe9QUUO9q"
      },
      "source": [
        "### Similaridade de Textos\n",
        "\n",
        "A comparação entre textos é essencial em várias aplicações de NLP, como procura, deduplicação, e detecção de semântica semelhante. A similaridade pode ser calculada de forma vetorial (com BoW, TF-IDF, BERT) ou diretamente entre strings. A biblioteca FuzzyWuzzy facilita essa tarefa ao implementar métricas clássicas como a distância de Levenshtein, oferecendo métodos que consideram variações de ordem, duplicações e substrings. Além dela, há outras métricas relevantes como Jaccard, Jaro e Jaro-Winkler, que permitem medir o grau de similaridade com maior flexibilidade e precisão do que simples comparações binárias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_jqOvnLUw26"
      },
      "outputs": [],
      "source": [
        "!pip install transformers fuzzywuzzy\n",
        "\n",
        "# Importação da biblioteca\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Frases de exemplo\n",
        "texto1 = \"Eu gosto de café\"\n",
        "texto2 = \"Eu gosto de café\"\n",
        "texto3 = \"Eu não gosto de café\"\n",
        "texto4 = \"Eu gosto de café e meu irmão também\"\n",
        "texto5 = \"De café eu gosto\"\n",
        "texto6 = \"Eu eu gosto gosto de de café café\"\n",
        "\n",
        "print(\"=== Comparações com FuzzyWuzzy ===\")\n",
        "\n",
        "# Comparação direta (match exato)\n",
        "print(\"\\n1. Ratio (match exato)\")\n",
        "print(f\"fuzz.ratio('{texto1}', '{texto2}') = {fuzz.ratio(texto1, texto2)}\")  # 100\n",
        "\n",
        "print(\"\\n2. Ratio (quase igual, com negação)\")\n",
        "print(f\"fuzz.ratio('{texto1}', '{texto3}') = {fuzz.ratio(texto1, texto3)}\")  # ~89\n",
        "\n",
        "# Comparação considerando substrings\n",
        "print(\"\\n3. Partial Ratio (substring)\")\n",
        "print(f\"fuzz.partial_ratio('{texto1}', '{texto4}') = {fuzz.partial_ratio(texto1, texto4)}\")  # 100\n",
        "\n",
        "# Comparação com ordem diferente das palavras\n",
        "print(\"\\n4. Token Sort Ratio (ordem diferente)\")\n",
        "print(f\"fuzz.token_sort_ratio('{texto1}', '{texto5}') = {fuzz.token_sort_ratio(texto1, texto5)}\")  # 100\n",
        "\n",
        "# Comparação ignorando duplicatas\n",
        "print(\"\\n5. Token Set Ratio (duplicatas)\")\n",
        "print(f\"fuzz.token_set_ratio('{texto1}', '{texto6}') = {fuzz.token_set_ratio(texto1, texto6)}\")  # 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxMs49pVWf98"
      },
      "source": [
        "### Similaridade de Cosseno sem Contexto\n",
        "\n",
        "A Similaridade de Cosseno é uma métrica vetorial amplamente usada em NLP para medir o grau de similaridade entre textos, calculando o cosseno do ângulo entre seus vetores representados em um espaço N-dimensional. Ao aplicar técnicas como TF-IDF para transformar textos em vetores, é possível usar cosine_similarity do Scikit-learn para comparar conteúdos. Embora eficaz, essa abordagem não considera o contexto semântico das palavras, focando apenas na distribuição e frequência dos termos. É comum em sistemas de busca e recomendação baseados em conteúdo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LovVQFAOWt_0"
      },
      "outputs": [],
      "source": [
        "# Instalar o scikit-learn (se necessário)\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Lista de textos em português\n",
        "text_list = [\n",
        "    \"Fui ao shopping\",\n",
        "    \"Hoje fui ao shopping\",\n",
        "    \"Gosto de café\"\n",
        "]\n",
        "\n",
        "# Vetorização com TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vectorizer.fit_transform(text_list)\n",
        "\n",
        "# Cálculo da similaridade de cosseno entre todos os pares\n",
        "cosine_sim = cosine_similarity(text_tfidf, text_tfidf)\n",
        "\n",
        "# Exibição da matriz de similaridade com rótulos\n",
        "df_sim = pd.DataFrame(cosine_sim, index=text_list, columns=text_list)\n",
        "print(\"Matriz de Similaridade de Cosseno:\")\n",
        "print(df_sim.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SecfQFuyW-KL"
      },
      "source": [
        "### Similaridade de Cosseno com Contexto\n",
        "\n",
        "A Similaridade de Cosseno com contexto utiliza modelos como o BERT (ou variações, como paraphrase-xlm-r-multilingual-v1) para gerar vetores que capturam o significado contextual dos textos. Ao aplicar a métrica de cosseno sobre esses vetores, é possível medir com precisão a semelhança semântica entre frases, mesmo quando expressas com palavras diferentes. Essa abordagem supera métodos tradicionais ao considerar o sentido das palavras no enunciado, sendo ideal para tarefas como detecção de paráfrases, busca semântica e agrupamento de ideias semelhantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34SyLzztXLmb"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Carrega o modelo multilíngue com suporte ao português\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# Lista de textos em português\n",
        "text_list = [\n",
        "    \"Eu moro em Portugal\",\n",
        "    \"A minha casa fica em território português\",\n",
        "    \"Gosto de viajar de avião\"\n",
        "]\n",
        "\n",
        "# Geração dos embeddings com contexto\n",
        "text_bert = model.encode(text_list, show_progress_bar=True)\n",
        "\n",
        "# Cálculo da similaridade de cosseno entre os textos\n",
        "cosine_sim = cosine_similarity(text_bert, text_bert)\n",
        "\n",
        "# Exibição da matriz de similaridade com rótulos\n",
        "df_sim = pd.DataFrame(cosine_sim, index=text_list, columns=text_list)\n",
        "print(\"Matriz de Similaridade com Contexto (BERT):\")\n",
        "print(df_sim.round(2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_basics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
